{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "closed-funds",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-25 16:06:56.131423: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-25 16:06:58.341310: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/mesa-diverted/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/mesa:/usr/lib/x86_64-linux-gnu/dri:/usr/lib/x86_64-linux-gnu/gallium-pipe:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-25 16:06:58.341534: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/mesa-diverted/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/mesa:/usr/lib/x86_64-linux-gnu/dri:/usr/lib/x86_64-linux-gnu/gallium-pipe:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-25 16:06:58.341550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../../fastshap_tf/')\n",
    "from fastshap import ImageFastSHAP, ShapleySampler, ResizeMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comparable-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import shap\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "insured-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Input, Layer, Dense, Lambda, Reshape, Multiply)\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "understood-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "antique-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "os.environ['PYTHONHASHSEED'] = str(420)\n",
    "import random\n",
    "random.seed(420)\n",
    "np.random.seed(420)\n",
    "tf.random.set_seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-adobe",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spanish-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LR = 1e-3\n",
    "INPUT_SHAPE = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intelligent-guinea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T19:34:36.732265Z",
     "start_time": "2021-03-16T19:34:36.267950Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 train samples\n",
      "5000 val samples\n",
      "5000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 16:07:01.797107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:07:02.086575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:07:02.087823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:07:02.094800: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-25 16:07:02.103668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:07:02.104953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:07:02.106002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:07:04.558193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:07:04.561864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:07:04.563451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:07:04.565789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11424 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2023-05-25 16:07:05.176123: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 614400000 exceeds 10% of free system memory.\n",
      "2023-05-25 16:07:05.948871: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 614400000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, train_size=0.5, random_state=420)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#Resize to 224x224\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_val.shape[0], 'val samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Make TF Dataset\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-antibody",
   "metadata": {},
   "source": [
    "### Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mathematical-cartridge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T19:34:38.036975Z",
     "start_time": "2021-03-16T19:34:38.030145Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_data(dataset, fn, batch_size=32):\n",
    "    dataset = dataset.map(fn)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-nature",
   "metadata": {},
   "source": [
    "### Reformat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hungry-master",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T19:34:39.459330Z",
     "start_time": "2021-03-16T19:34:39.356585Z"
    }
   },
   "outputs": [],
   "source": [
    "def reformat(x, y):\n",
    "    \n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x = Resizing(INPUT_SHAPE[0], INPUT_SHAPE[1], interpolation='nearest')(x)\n",
    "    x = tf.keras.applications.resnet50.preprocess_input(x)\n",
    "    \n",
    "    return (x, y)\n",
    "\n",
    "ds_train = batch_data(ds_train, reformat, BATCH_SIZE)\n",
    "ds_val = batch_data(ds_val, reformat, BATCH_SIZE)\n",
    "ds_test = batch_data(ds_test, reformat, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-foster",
   "metadata": {},
   "source": [
    "## Load Surrogate Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "encouraging-digit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UniquePtr.__del__ at 0x7f615afdcd30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/framework/c_api_util.py\", line 70, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "P = 14*14\n",
    "value_model = ResNet50(\n",
    "    include_top=False, weights='imagenet', \n",
    "    input_shape=INPUT_SHAPE, pooling='avg'\n",
    ") \n",
    "D = 10\n",
    "\n",
    "model_input = Input(shape=INPUT_SHAPE, dtype='float64', name='input')\n",
    "S = ShapleySampler(P, paired_sampling=False, num_samples=1)(model_input)\n",
    "S = Lambda(lambda x: tf.cast(x, tf.float32))(S)\n",
    "S = Reshape((P,))(S)\n",
    "S = ResizeMask(in_shape=INPUT_SHAPE, mask_size=P)(S)\n",
    "xs = Multiply()([model_input, S])\n",
    "\n",
    "net = value_model(xs)\n",
    "out = Dense(D, activation='softmax')(net)\n",
    "\n",
    "surrogate = Model(model_input, out)\n",
    "\n",
    "# Get Checkpointed Model\n",
    "weights_path = 'surrogate/20221115_02_04_39/value_weights.h5'\n",
    "surrogate.load_weights(weights_path)\n",
    "\n",
    "# Remove Masking Layer\n",
    "surrogate = Sequential(   \n",
    "    [l for l in surrogate.layers[-2:]]\n",
    ")\n",
    "surrogate.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-diving",
   "metadata": {},
   "source": [
    "# Train FastSHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-wildlife",
   "metadata": {},
   "source": [
    "### Save Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y%m%d_%H_%M_%S\")\n",
    "save_dir = 'fastshap'\n",
    "model_dir = os.path.join(os.getcwd(), save_dir, date)\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-observation",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import fastshap\n",
    "import utils\n",
    "reload(fastshap)\n",
    "reload(utils)\n",
    "from fastshap import ImageFastSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-fraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 05:33:00.083454: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n"
     ]
    }
   ],
   "source": [
    "fastshap = ImageFastSHAP(imputer = surrogate,\n",
    "                         normalization=None,\n",
    "                         model_dir = model_dir, \n",
    "                         link='logit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-inside",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-battlefield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_5), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32>\n",
      "  <tf.Variable 'conv1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_0_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_0_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_0_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_0_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_1_conv/kernel:0' shape=(1, 1, 64, 64) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_2_conv/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_2_conv/bias:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_2_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_2_bn/beta:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_3_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_3_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_3_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block1_3_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_1_conv/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_2_conv/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_2_conv/bias:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_2_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_2_bn/beta:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_3_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_3_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_3_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block2_3_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_1_conv/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_1_conv/bias:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_1_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_1_bn/beta:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_2_conv/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_2_conv/bias:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_2_bn/gamma:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_2_bn/beta:0' shape=(64,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_3_conv/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_3_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_3_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv2_block3_3_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_0_conv/kernel:0' shape=(1, 1, 256, 512) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_0_conv/bias:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_0_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_0_bn/beta:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_1_conv/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block1_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_1_conv/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block2_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_1_conv/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block3_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_1_conv/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_1_conv/bias:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_1_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_1_bn/beta:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_2_conv/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_2_conv/bias:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_2_bn/gamma:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_2_bn/beta:0' shape=(128,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_3_conv/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_3_conv/bias:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_3_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv3_block4_3_bn/beta:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_0_conv/kernel:0' shape=(1, 1, 512, 1024) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_0_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_0_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_0_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_1_conv/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block1_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block2_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block3_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block4_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block5_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_1_conv/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_1_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_1_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_1_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_2_conv/bias:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_2_bn/gamma:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_2_bn/beta:0' shape=(256,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_3_conv/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_3_conv/bias:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_3_bn/gamma:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv4_block6_3_bn/beta:0' shape=(1024,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_0_conv/kernel:0' shape=(1, 1, 1024, 2048) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_0_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_0_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_0_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_1_conv/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_1_conv/bias:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_1_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_1_bn/beta:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_2_conv/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_2_conv/bias:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_2_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_2_bn/beta:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_3_conv/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_3_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_3_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block1_3_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_1_conv/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_1_conv/bias:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_1_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_1_bn/beta:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_2_conv/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_2_conv/bias:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_2_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_2_bn/beta:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_3_conv/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_3_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_3_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block2_3_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_1_conv/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_1_conv/bias:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_1_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_1_bn/beta:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_2_conv/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_2_conv/bias:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_2_bn/gamma:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_2_bn/beta:0' shape=(512,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_3_conv/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_3_conv/bias:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_3_bn/gamma:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'conv5_block3_3_bn/beta:0' shape=(2048,) dtype=float32>\n",
      "  <tf.Variable 'dense/kernel:0' shape=(2048, 10) dtype=float32>\n",
      "  <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 74.7973 - shap_loss: 74.7973 - eff_loss: 0.0000e+00\n",
      "Epoch 1: val_shap_loss improved from inf to 28.58216, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 840s 531ms/step - loss: 74.7973 - shap_loss: 74.7973 - eff_loss: 0.0000e+00 - val_loss: 28.5822 - val_shap_loss: 28.5822 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 26.3697 - shap_loss: 26.3697 - eff_loss: 0.0000e+00\n",
      "Epoch 2: val_shap_loss improved from 28.58216 to 24.03773, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 828s 529ms/step - loss: 26.3697 - shap_loss: 26.3697 - eff_loss: 0.0000e+00 - val_loss: 24.0377 - val_shap_loss: 24.0377 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 22.9350 - shap_loss: 22.9350 - eff_loss: 0.0000e+00\n",
      "Epoch 3: val_shap_loss improved from 24.03773 to 23.22303, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 828s 530ms/step - loss: 22.9350 - shap_loss: 22.9350 - eff_loss: 0.0000e+00 - val_loss: 23.2230 - val_shap_loss: 23.2230 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 21.2897 - shap_loss: 21.2897 - eff_loss: 0.0000e+00\n",
      "Epoch 4: val_shap_loss improved from 23.22303 to 21.54141, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 826s 528ms/step - loss: 21.2897 - shap_loss: 21.2897 - eff_loss: 0.0000e+00 - val_loss: 21.5414 - val_shap_loss: 21.5414 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 20.2600 - shap_loss: 20.2600 - eff_loss: 0.0000e+00\n",
      "Epoch 5: val_shap_loss improved from 21.54141 to 20.28758, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 826s 529ms/step - loss: 20.2600 - shap_loss: 20.2600 - eff_loss: 0.0000e+00 - val_loss: 20.2876 - val_shap_loss: 20.2876 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 19.3967 - shap_loss: 19.3967 - eff_loss: 0.0000e+00\n",
      "Epoch 6: val_shap_loss improved from 20.28758 to 19.87218, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 826s 529ms/step - loss: 19.3967 - shap_loss: 19.3967 - eff_loss: 0.0000e+00 - val_loss: 19.8722 - val_shap_loss: 19.8722 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 18.7891 - shap_loss: 18.7891 - eff_loss: 0.0000e+00\n",
      "Epoch 7: val_shap_loss improved from 19.87218 to 18.66051, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 18.7891 - shap_loss: 18.7891 - eff_loss: 0.0000e+00 - val_loss: 18.6605 - val_shap_loss: 18.6605 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 18.3540 - shap_loss: 18.3540 - eff_loss: 0.0000e+00\n",
      "Epoch 8: val_shap_loss improved from 18.66051 to 18.33986, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 18.3540 - shap_loss: 18.3540 - eff_loss: 0.0000e+00 - val_loss: 18.3399 - val_shap_loss: 18.3399 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 17.8160 - shap_loss: 17.8160 - eff_loss: 0.0000e+00\n",
      "Epoch 9: val_shap_loss did not improve from 18.33986\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 17.8160 - shap_loss: 17.8160 - eff_loss: 0.0000e+00 - val_loss: 19.4082 - val_shap_loss: 19.4082 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 17.6113 - shap_loss: 17.6113 - eff_loss: 0.0000e+00\n",
      "Epoch 10: val_shap_loss improved from 18.33986 to 17.86730, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 17.6113 - shap_loss: 17.6113 - eff_loss: 0.0000e+00 - val_loss: 17.8673 - val_shap_loss: 17.8673 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 17.2707 - shap_loss: 17.2707 - eff_loss: 0.0000e+00\n",
      "Epoch 11: val_shap_loss improved from 17.86730 to 17.35303, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 17.2707 - shap_loss: 17.2707 - eff_loss: 0.0000e+00 - val_loss: 17.3530 - val_shap_loss: 17.3530 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 16.9854 - shap_loss: 16.9854 - eff_loss: 0.0000e+00\n",
      "Epoch 12: val_shap_loss improved from 17.35303 to 17.19446, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 16.9854 - shap_loss: 16.9854 - eff_loss: 0.0000e+00 - val_loss: 17.1945 - val_shap_loss: 17.1945 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 16.7416 - shap_loss: 16.7416 - eff_loss: 0.0000e+00\n",
      "Epoch 13: val_shap_loss did not improve from 17.19446\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 16.7416 - shap_loss: 16.7416 - eff_loss: 0.0000e+00 - val_loss: 18.2696 - val_shap_loss: 18.2696 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 16.6335 - shap_loss: 16.6335 - eff_loss: 0.0000e+00\n",
      "Epoch 14: val_shap_loss did not improve from 17.19446\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 16.6335 - shap_loss: 16.6335 - eff_loss: 0.0000e+00 - val_loss: 17.3996 - val_shap_loss: 17.3996 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 16.4715 - shap_loss: 16.4715 - eff_loss: 0.0000e+00\n",
      "Epoch 15: val_shap_loss did not improve from 17.19446\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 16.4715 - shap_loss: 16.4715 - eff_loss: 0.0000e+00 - val_loss: 17.5723 - val_shap_loss: 17.5723 - val_eff_loss: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 16.0876 - shap_loss: 16.0876 - eff_loss: 0.0000e+00\n",
      "Epoch 16: val_shap_loss improved from 17.19446 to 16.77092, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 16.0876 - shap_loss: 16.0876 - eff_loss: 0.0000e+00 - val_loss: 16.7709 - val_shap_loss: 16.7709 - val_eff_loss: 0.0000e+00 - lr: 8.0000e-04\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 15.8920 - shap_loss: 15.8920 - eff_loss: 0.0000e+00\n",
      "Epoch 17: val_shap_loss did not improve from 16.77092\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 15.8920 - shap_loss: 15.8920 - eff_loss: 0.0000e+00 - val_loss: 16.8628 - val_shap_loss: 16.8628 - val_eff_loss: 0.0000e+00 - lr: 8.0000e-04\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 15.9457 - shap_loss: 15.9457 - eff_loss: 0.0000e+00\n",
      "Epoch 18: val_shap_loss improved from 16.77092 to 16.70138, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 15.9457 - shap_loss: 15.9457 - eff_loss: 0.0000e+00 - val_loss: 16.7014 - val_shap_loss: 16.7014 - val_eff_loss: 0.0000e+00 - lr: 8.0000e-04\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 15.8065 - shap_loss: 15.8065 - eff_loss: 0.0000e+00\n",
      "Epoch 19: val_shap_loss did not improve from 16.70138\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 15.8065 - shap_loss: 15.8065 - eff_loss: 0.0000e+00 - val_loss: 17.3223 - val_shap_loss: 17.3223 - val_eff_loss: 0.0000e+00 - lr: 8.0000e-04\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 15.6932 - shap_loss: 15.6932 - eff_loss: 0.0000e+00\n",
      "Epoch 20: val_shap_loss did not improve from 16.70138\n",
      "1563/1563 [==============================] - 854s 546ms/step - loss: 15.6932 - shap_loss: 15.6932 - eff_loss: 0.0000e+00 - val_loss: 17.1533 - val_shap_loss: 17.1533 - val_eff_loss: 0.0000e+00 - lr: 8.0000e-04\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 15.4745 - shap_loss: 15.4745 - eff_loss: 0.0000e+00\n",
      "Epoch 21: val_shap_loss did not improve from 16.70138\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 15.4745 - shap_loss: 15.4745 - eff_loss: 0.0000e+00 - val_loss: 17.4183 - val_shap_loss: 17.4183 - val_eff_loss: 0.0000e+00 - lr: 8.0000e-04\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 15.3686 - shap_loss: 15.3686 - eff_loss: 0.0000e+00\n",
      "Epoch 22: val_shap_loss improved from 16.70138 to 16.34556, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 15.3686 - shap_loss: 15.3686 - eff_loss: 0.0000e+00 - val_loss: 16.3456 - val_shap_loss: 16.3456 - val_eff_loss: 0.0000e+00 - lr: 6.4000e-04\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 15.2637 - shap_loss: 15.2637 - eff_loss: 0.0000e+00\n",
      "Epoch 23: val_shap_loss did not improve from 16.34556\n",
      "1563/1563 [==============================] - 853s 546ms/step - loss: 15.2637 - shap_loss: 15.2637 - eff_loss: 0.0000e+00 - val_loss: 16.7986 - val_shap_loss: 16.7986 - val_eff_loss: 0.0000e+00 - lr: 6.4000e-04\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 15.2335 - shap_loss: 15.2335 - eff_loss: 0.0000e+00\n",
      "Epoch 24: val_shap_loss did not improve from 16.34556\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 15.2335 - shap_loss: 15.2335 - eff_loss: 0.0000e+00 - val_loss: 17.0375 - val_shap_loss: 17.0375 - val_eff_loss: 0.0000e+00 - lr: 6.4000e-04\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 15.0968 - shap_loss: 15.0968 - eff_loss: 0.0000e+00\n",
      "Epoch 25: val_shap_loss did not improve from 16.34556\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 15.0968 - shap_loss: 15.0968 - eff_loss: 0.0000e+00 - val_loss: 16.5812 - val_shap_loss: 16.5812 - val_eff_loss: 0.0000e+00 - lr: 6.4000e-04\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 15.1337 - shap_loss: 15.1337 - eff_loss: 0.0000e+00\n",
      "Epoch 26: val_shap_loss did not improve from 16.34556\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 15.1337 - shap_loss: 15.1337 - eff_loss: 0.0000e+00 - val_loss: 16.4445 - val_shap_loss: 16.4445 - val_eff_loss: 0.0000e+00 - lr: 5.1200e-04\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.8647 - shap_loss: 14.8647 - eff_loss: 0.0000e+00\n",
      "Epoch 27: val_shap_loss did not improve from 16.34556\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.8647 - shap_loss: 14.8647 - eff_loss: 0.0000e+00 - val_loss: 16.7680 - val_shap_loss: 16.7680 - val_eff_loss: 0.0000e+00 - lr: 5.1200e-04\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.8118 - shap_loss: 14.8118 - eff_loss: 0.0000e+00\n",
      "Epoch 28: val_shap_loss did not improve from 16.34556\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.8118 - shap_loss: 14.8118 - eff_loss: 0.0000e+00 - val_loss: 16.5045 - val_shap_loss: 16.5045 - val_eff_loss: 0.0000e+00 - lr: 5.1200e-04\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.7174 - shap_loss: 14.7174 - eff_loss: 0.0000e+00\n",
      "Epoch 29: val_shap_loss did not improve from 16.34556\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.7174 - shap_loss: 14.7174 - eff_loss: 0.0000e+00 - val_loss: 16.5583 - val_shap_loss: 16.5583 - val_eff_loss: 0.0000e+00 - lr: 4.0960e-04\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.7501 - shap_loss: 14.7501 - eff_loss: 0.0000e+00\n",
      "Epoch 30: val_shap_loss improved from 16.34556 to 15.94199, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 14.7501 - shap_loss: 14.7501 - eff_loss: 0.0000e+00 - val_loss: 15.9420 - val_shap_loss: 15.9420 - val_eff_loss: 0.0000e+00 - lr: 4.0960e-04\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.5992 - shap_loss: 14.5992 - eff_loss: 0.0000e+00\n",
      "Epoch 31: val_shap_loss did not improve from 15.94199\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.5992 - shap_loss: 14.5992 - eff_loss: 0.0000e+00 - val_loss: 16.3085 - val_shap_loss: 16.3085 - val_eff_loss: 0.0000e+00 - lr: 4.0960e-04\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.7032 - shap_loss: 14.7032 - eff_loss: 0.0000e+00\n",
      "Epoch 32: val_shap_loss did not improve from 15.94199\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 14.7032 - shap_loss: 14.7032 - eff_loss: 0.0000e+00 - val_loss: 16.7602 - val_shap_loss: 16.7602 - val_eff_loss: 0.0000e+00 - lr: 4.0960e-04\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.5956 - shap_loss: 14.5956 - eff_loss: 0.0000e+00\n",
      "Epoch 33: val_shap_loss did not improve from 15.94199\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.5956 - shap_loss: 14.5956 - eff_loss: 0.0000e+00 - val_loss: 16.3718 - val_shap_loss: 16.3718 - val_eff_loss: 0.0000e+00 - lr: 4.0960e-04\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.5262 - shap_loss: 14.5262 - eff_loss: 0.0000e+00\n",
      "Epoch 34: val_shap_loss did not improve from 15.94199\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.5262 - shap_loss: 14.5262 - eff_loss: 0.0000e+00 - val_loss: 16.3327 - val_shap_loss: 16.3327 - val_eff_loss: 0.0000e+00 - lr: 3.2768e-04\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.5472 - shap_loss: 14.5472 - eff_loss: 0.0000e+00\n",
      "Epoch 35: val_shap_loss did not improve from 15.94199\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 14.5472 - shap_loss: 14.5472 - eff_loss: 0.0000e+00 - val_loss: 16.8732 - val_shap_loss: 16.8732 - val_eff_loss: 0.0000e+00 - lr: 3.2768e-04\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.4063 - shap_loss: 14.4063 - eff_loss: 0.0000e+00\n",
      "Epoch 36: val_shap_loss did not improve from 15.94199\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "1563/1563 [==============================] - 823s 527ms/step - loss: 14.4063 - shap_loss: 14.4063 - eff_loss: 0.0000e+00 - val_loss: 15.9679 - val_shap_loss: 15.9679 - val_eff_loss: 0.0000e+00 - lr: 3.2768e-04\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.3038 - shap_loss: 14.3038 - eff_loss: 0.0000e+00\n",
      "Epoch 37: val_shap_loss did not improve from 15.94199\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.3038 - shap_loss: 14.3038 - eff_loss: 0.0000e+00 - val_loss: 16.2249 - val_shap_loss: 16.2249 - val_eff_loss: 0.0000e+00 - lr: 2.6214e-04\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.3926 - shap_loss: 14.3926 - eff_loss: 0.0000e+00\n",
      "Epoch 38: val_shap_loss did not improve from 15.94199\n",
      "1563/1563 [==============================] - 823s 527ms/step - loss: 14.3926 - shap_loss: 14.3926 - eff_loss: 0.0000e+00 - val_loss: 16.3235 - val_shap_loss: 16.3235 - val_eff_loss: 0.0000e+00 - lr: 2.6214e-04\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.3711 - shap_loss: 14.3711 - eff_loss: 0.0000e+00\n",
      "Epoch 39: val_shap_loss did not improve from 15.94199\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.3711 - shap_loss: 14.3711 - eff_loss: 0.0000e+00 - val_loss: 16.4066 - val_shap_loss: 16.4066 - val_eff_loss: 0.0000e+00 - lr: 2.6214e-04\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.2807 - shap_loss: 14.2807 - eff_loss: 0.0000e+00\n",
      "Epoch 40: val_shap_loss improved from 15.94199 to 15.59749, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/fastshap/20221116_05_32_57/explainer_weights.h5\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 14.2807 - shap_loss: 14.2807 - eff_loss: 0.0000e+00 - val_loss: 15.5975 - val_shap_loss: 15.5975 - val_eff_loss: 0.0000e+00 - lr: 2.0972e-04\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.1740 - shap_loss: 14.1740 - eff_loss: 0.0000e+00\n",
      "Epoch 41: val_shap_loss did not improve from 15.59749\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.1740 - shap_loss: 14.1740 - eff_loss: 0.0000e+00 - val_loss: 15.6763 - val_shap_loss: 15.6763 - val_eff_loss: 0.0000e+00 - lr: 2.0972e-04\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.2409 - shap_loss: 14.2409 - eff_loss: 0.0000e+00\n",
      "Epoch 42: val_shap_loss did not improve from 15.59749\n",
      "1563/1563 [==============================] - 825s 528ms/step - loss: 14.2409 - shap_loss: 14.2409 - eff_loss: 0.0000e+00 - val_loss: 16.1123 - val_shap_loss: 16.1123 - val_eff_loss: 0.0000e+00 - lr: 2.0972e-04\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.3334 - shap_loss: 14.3334 - eff_loss: 0.0000e+00\n",
      "Epoch 43: val_shap_loss did not improve from 15.59749\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.3334 - shap_loss: 14.3334 - eff_loss: 0.0000e+00 - val_loss: 16.1098 - val_shap_loss: 16.1098 - val_eff_loss: 0.0000e+00 - lr: 2.0972e-04\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.1864 - shap_loss: 14.1864 - eff_loss: 0.0000e+00\n",
      "Epoch 44: val_shap_loss did not improve from 15.59749\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.1864 - shap_loss: 14.1864 - eff_loss: 0.0000e+00 - val_loss: 16.2063 - val_shap_loss: 16.2063 - val_eff_loss: 0.0000e+00 - lr: 1.6777e-04\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.1051 - shap_loss: 14.1051 - eff_loss: 0.0000e+00\n",
      "Epoch 45: val_shap_loss did not improve from 15.59749\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.1051 - shap_loss: 14.1051 - eff_loss: 0.0000e+00 - val_loss: 15.7640 - val_shap_loss: 15.7640 - val_eff_loss: 0.0000e+00 - lr: 1.6777e-04\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.0788 - shap_loss: 14.0788 - eff_loss: 0.0000e+00\n",
      "Epoch 46: val_shap_loss did not improve from 15.59749\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.0788 - shap_loss: 14.0788 - eff_loss: 0.0000e+00 - val_loss: 15.7203 - val_shap_loss: 15.7203 - val_eff_loss: 0.0000e+00 - lr: 1.6777e-04\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.0963 - shap_loss: 14.0963 - eff_loss: 0.0000e+00\n",
      "Epoch 47: val_shap_loss did not improve from 15.59749\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.0963 - shap_loss: 14.0963 - eff_loss: 0.0000e+00 - val_loss: 15.9227 - val_shap_loss: 15.9227 - val_eff_loss: 0.0000e+00 - lr: 1.3422e-04\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.1188 - shap_loss: 14.1188 - eff_loss: 0.0000e+00\n",
      "Epoch 48: val_shap_loss did not improve from 15.59749\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.1188 - shap_loss: 14.1188 - eff_loss: 0.0000e+00 - val_loss: 15.8869 - val_shap_loss: 15.8869 - val_eff_loss: 0.0000e+00 - lr: 1.3422e-04\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.1151 - shap_loss: 14.1151 - eff_loss: 0.0000e+00\n",
      "Epoch 49: val_shap_loss did not improve from 15.59749\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.1151 - shap_loss: 14.1151 - eff_loss: 0.0000e+00 - val_loss: 16.0798 - val_shap_loss: 16.0798 - val_eff_loss: 0.0000e+00 - lr: 1.3422e-04\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 14.0793 - shap_loss: 14.0793 - eff_loss: 0.0000e+00\n",
      "Epoch 50: val_shap_loss did not improve from 15.59749\n",
      "1563/1563 [==============================] - 824s 527ms/step - loss: 14.0793 - shap_loss: 14.0793 - eff_loss: 0.0000e+00 - val_loss: 16.1294 - val_shap_loss: 16.1294 - val_eff_loss: 0.0000e+00 - lr: 1.0737e-04\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " model_3 (Functional)        (None, 14, 14, 256)       4955520   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 10)        2570      \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 196, 10)           0         \n",
      "                                                                 \n",
      " permute_1 (Permute)         (None, 10, 196)           0         \n",
      "                                                                 \n",
      " phi (Layer)                 (None, 10, 196)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,958,090\n",
      "Trainable params: 4,939,274\n",
      "Non-trainable params: 18,816\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "fastshap.train(train_data = ds_train, \n",
    "              val_data = ds_val, \n",
    "              max_epochs = EPOCHS, \n",
    "              batch_size = BATCH_SIZE, \n",
    "              num_samples = 1,\n",
    "              lr = LR,\n",
    "              paired_sampling = False, \n",
    "              eff_lambda = 0.0,\n",
    "              verbose = 1,\n",
    "              lookback = 10)\n",
    "training_time = time.time() - t\n",
    "\n",
    "with open(os.path.join(model_dir, 'training_time.pkl'), 'wb') as f:\n",
    "    pickle.dump(training_time, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-cross",
   "metadata": {},
   "source": [
    "# Explain w/ FastSHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-murder",
   "metadata": {},
   "source": [
    "### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = os.path.join(os.getcwd(), 'images')\n",
    "images = np.load(os.path.join(images_dir, 'processed_images.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a5b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 224, 224, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-liberal",
   "metadata": {},
   "source": [
    "### Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7f361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "#from keras import backend as K\n",
    "#tf.compat.v1.keras.backend.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-lying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "shap_values = fastshap.explainer.predict(images)\n",
    "explaining_time = time.time() - t\n",
    "shap_values = [shap_values[:,:,:,i] for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-extension",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = 'fastshap'\n",
    "# model_dir = os.path.join(os.getcwd(), save_dir, 'results')\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "with open(os.path.join(model_dir, 'explaining_time.pkl'), 'wb') as f:\n",
    "    pickle.dump(explaining_time, f)\n",
    "    \n",
    "with open(os.path.join(model_dir, 'shap_values.pkl'), 'wb') as f:\n",
    "    pickle.dump(shap_values, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b00ad1606bbcc2f06f6d0dddae4ba1f9f3abf61f006d5f3ac83db30fb57b2e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
