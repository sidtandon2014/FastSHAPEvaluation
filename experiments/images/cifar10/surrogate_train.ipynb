{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ahead-endorsement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-25 16:11:41.037215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-25 16:11:42.192767: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/mesa-diverted/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/mesa:/usr/lib/x86_64-linux-gnu/dri:/usr/lib/x86_64-linux-gnu/gallium-pipe:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-25 16:11:42.192890: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/mesa-diverted/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/mesa:/usr/lib/x86_64-linux-gnu/dri:/usr/lib/x86_64-linux-gnu/gallium-pipe:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-25 16:11:42.192902: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../../fastshap_tf/')\n",
    "from surrogate import ImageSurrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comparable-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import shap\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacterial-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Input, Layer, Dense, Lambda, \n",
    "                                     Dropout, Multiply, BatchNormalization, \n",
    "                                     Reshape, Concatenate, Conv2D, Permute)\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reasonable-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "os.environ['PYTHONHASHSEED'] = str(420)\n",
    "import random\n",
    "random.seed(420)\n",
    "np.random.seed(420)\n",
    "tf.random.set_seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "environmental-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4900bfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tensorflow as tf\\ntf.debugging.set_log_device_placement(True)\\n\\n# Create some tensors\\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\\nc = tf.matmul(a, b)\\n\\nprint(c)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-adobe",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "crude-ready",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T19:34:35.186987Z",
     "start_time": "2021-03-16T19:34:35.180960Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LR = 1e-3\n",
    "INPUT_SHAPE = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-module",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intelligent-guinea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T19:34:36.732265Z",
     "start_time": "2021-03-16T19:34:36.267950Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 train samples\n",
      "5000 val samples\n",
      "5000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 16:11:46.331505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:11:46.347425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:11:46.348125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:11:46.368184: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-25 16:11:46.369217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:11:46.369776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:11:46.370134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:11:47.198114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:11:47.198631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:11:47.198979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-25 16:11:47.199271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 548 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2023-05-25 16:11:48.177561: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 614400000 exceeds 10% of free system memory.\n",
      "2023-05-25 16:11:58.846792: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 585.94MiB (rounded to 614400000)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-05-25 16:11:58.846831: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for GPU_0_bfc\n",
      "2023-05-25 16:11:58.846841: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846848: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846855: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846861: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846867: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846874: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846880: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846886: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846892: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846898: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846905: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846911: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846917: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846924: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846930: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846936: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846943: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846949: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846957: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846964: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846970: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-05-25 16:11:58.846978: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 585.94MiB was 256.00MiB, Chunk State: \n",
      "2023-05-25 16:11:58.846984: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: \n",
      "2023-05-25 16:11:58.846990: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 0B\n",
      "2023-05-25 16:11:58.846995: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 0 memory_limit_: 575406080 available bytes: 575406080 curr_region_allocation_bytes_: 575406080\n",
      "2023-05-25 16:11:58.847005: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: \n",
      "Limit:                       575406080\n",
      "InUse:                               0\n",
      "MaxInUse:                            0\n",
      "NumAllocs:                           0\n",
      "MaxAllocSize:                        0\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-05-25 16:11:58.847011: W tensorflow/tsl/framework/bfc_allocator.cc:492] <allocator contains no memory>\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m y_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(y_test, num_classes)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Make TF Dataset\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m ds_train \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m ds_val \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((x_val, y_val))\n\u001b[1;32m     26\u001b[0m ds_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((x_test, y_test))\n",
      "File \u001b[0;32m~/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:818\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[39m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m from_tensor_slices_op  \u001b[39m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m \u001b[39mreturn\u001b[39;00m from_tensor_slices_op\u001b[39m.\u001b[39;49mfrom_tensor_slices(tensors, name)\n",
      "File \u001b[0;32m~/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, element, is_files\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m   \u001b[39m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39;49mnormalize_element(element)\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m~/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:130\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(spec, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    129\u001b[0m         normalized_components\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 130\u001b[0m             ops\u001b[39m.\u001b[39;49mconvert_to_tensor(t, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcomponent_\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m i, dtype\u001b[39m=\u001b[39;49mdtype))\n\u001b[1;32m    131\u001b[0m \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[0;32m~/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1636\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1627\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1628\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1629\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1632\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1633\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1635\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1638\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1639\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     47\u001b[0m   \u001b[39mdel\u001b[39;00m as_ref  \u001b[39m# Unused.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m   \u001b[39mreturn\u001b[39;00m constant_op\u001b[39m.\u001b[39;49mconstant(value, dtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m   \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/Sid/GitRepo/iclr-fastshap/fastshap/.venv/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, train_size=0.5, random_state=420)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#Resize to 224x224\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_val.shape[0], 'val samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Make TF Dataset\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-antibody",
   "metadata": {},
   "source": [
    "### Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-cartridge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T19:34:38.036975Z",
     "start_time": "2021-03-16T19:34:38.030145Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_data(dataset, fn, batch_size=32):\n",
    "    dataset = dataset.map(fn)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-nature",
   "metadata": {},
   "source": [
    "### Reformat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-master",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T19:34:39.459330Z",
     "start_time": "2021-03-16T19:34:39.356585Z"
    }
   },
   "outputs": [],
   "source": [
    "def reformat(x, y):\n",
    "    \n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x = Resizing(INPUT_SHAPE[0], INPUT_SHAPE[1], interpolation='nearest')(x)\n",
    "    x = tf.keras.applications.resnet50.preprocess_input(x)\n",
    "    \n",
    "    return (x, y)\n",
    "\n",
    "ds_train = batch_data(ds_train, reformat, BATCH_SIZE)\n",
    "ds_val = batch_data(ds_val, reformat, BATCH_SIZE)\n",
    "ds_test = batch_data(ds_test, reformat, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-foster",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "base_model = ResNet50(\n",
    "    include_top=False, weights='imagenet', \n",
    "    input_shape=INPUT_SHAPE, pooling='avg'\n",
    ")\n",
    "base_model.trainable = True\n",
    "\n",
    "model_input = Input(shape=INPUT_SHAPE, name='input')\n",
    "\n",
    "net = base_model(model_input)\n",
    "out = Dense(10, activation='softmax')(net)\n",
    "\n",
    "model = Model(model_input, out)\n",
    "\n",
    "model_weights_path = 'model/20221113_03_18_29/model_weights.h5'\n",
    "\n",
    "model.load_weights(model_weights_path)\n",
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-fifth",
   "metadata": {},
   "source": [
    "# Train Surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-wildlife",
   "metadata": {},
   "source": [
    "### Save Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y%m%d_%H_%M_%S\")\n",
    "save_dir = 'surrogate'\n",
    "model_dir = os.path.join(os.getcwd(), save_dir, date)\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-clerk",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-canada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 02:04:54.114244: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - ETA: 0s - loss: 1.5318 - accuracy: 0.4478\n",
      "Epoch 1: val_loss improved from inf to 2.00007, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 713s 449ms/step - loss: 1.5318 - accuracy: 0.4478 - val_loss: 2.0001 - val_accuracy: 0.4080 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.2019 - accuracy: 0.5750\n",
      "Epoch 2: val_loss improved from 2.00007 to 1.23245, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 699s 448ms/step - loss: 1.2019 - accuracy: 0.5750 - val_loss: 1.2325 - val_accuracy: 0.5660 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.0724 - accuracy: 0.6199\n",
      "Epoch 3: val_loss improved from 1.23245 to 1.07516, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 699s 447ms/step - loss: 1.0724 - accuracy: 0.6199 - val_loss: 1.0752 - val_accuracy: 0.6266 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.0034 - accuracy: 0.6454\n",
      "Epoch 4: val_loss did not improve from 1.07516\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 1.0034 - accuracy: 0.6454 - val_loss: 1.1363 - val_accuracy: 0.6044 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.9456 - accuracy: 0.6649\n",
      "Epoch 5: val_loss did not improve from 1.07516\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.9456 - accuracy: 0.6649 - val_loss: 1.5519 - val_accuracy: 0.5638 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.9020 - accuracy: 0.6795\n",
      "Epoch 6: val_loss improved from 1.07516 to 0.99798, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 696s 446ms/step - loss: 0.9020 - accuracy: 0.6795 - val_loss: 0.9980 - val_accuracy: 0.6478 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.8492 - accuracy: 0.7024\n",
      "Epoch 7: val_loss improved from 0.99798 to 0.95167, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 696s 446ms/step - loss: 0.8492 - accuracy: 0.7024 - val_loss: 0.9517 - val_accuracy: 0.6726 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.8166 - accuracy: 0.7120\n",
      "Epoch 8: val_loss did not improve from 0.95167\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.8166 - accuracy: 0.7120 - val_loss: 1.0085 - val_accuracy: 0.6584 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.7907 - accuracy: 0.7229\n",
      "Epoch 9: val_loss did not improve from 0.95167\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.7907 - accuracy: 0.7229 - val_loss: 0.9562 - val_accuracy: 0.6814 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.7672 - accuracy: 0.7307\n",
      "Epoch 10: val_loss improved from 0.95167 to 0.91288, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.7672 - accuracy: 0.7307 - val_loss: 0.9129 - val_accuracy: 0.6940 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.7452 - accuracy: 0.7396\n",
      "Epoch 11: val_loss did not improve from 0.91288\n",
      "1563/1563 [==============================] - 695s 445ms/step - loss: 0.7452 - accuracy: 0.7396 - val_loss: 0.9319 - val_accuracy: 0.6854 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.7275 - accuracy: 0.7466\n",
      "Epoch 12: val_loss improved from 0.91288 to 0.88892, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.7275 - accuracy: 0.7466 - val_loss: 0.8889 - val_accuracy: 0.7030 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.7108 - accuracy: 0.7547\n",
      "Epoch 13: val_loss did not improve from 0.88892\n",
      "1563/1563 [==============================] - 695s 445ms/step - loss: 0.7108 - accuracy: 0.7547 - val_loss: 0.9600 - val_accuracy: 0.6802 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6993 - accuracy: 0.7564\n",
      "Epoch 14: val_loss did not improve from 0.88892\n",
      "1563/1563 [==============================] - 695s 444ms/step - loss: 0.6993 - accuracy: 0.7564 - val_loss: 0.9170 - val_accuracy: 0.7044 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6757 - accuracy: 0.7678\n",
      "Epoch 15: val_loss did not improve from 0.88892\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.6757 - accuracy: 0.7678 - val_loss: 0.8951 - val_accuracy: 0.7024 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6659 - accuracy: 0.7712\n",
      "Epoch 16: val_loss did not improve from 0.88892\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.6659 - accuracy: 0.7712 - val_loss: 1.0882 - val_accuracy: 0.6772 - lr: 9.0000e-04\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6542 - accuracy: 0.7749\n",
      "Epoch 17: val_loss improved from 0.88892 to 0.88175, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 696s 446ms/step - loss: 0.6542 - accuracy: 0.7749 - val_loss: 0.8817 - val_accuracy: 0.7136 - lr: 9.0000e-04\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6432 - accuracy: 0.7794\n",
      "Epoch 18: val_loss improved from 0.88175 to 0.88169, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.6432 - accuracy: 0.7794 - val_loss: 0.8817 - val_accuracy: 0.7058 - lr: 9.0000e-04\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6337 - accuracy: 0.7834\n",
      "Epoch 19: val_loss did not improve from 0.88169\n",
      "1563/1563 [==============================] - 695s 445ms/step - loss: 0.6337 - accuracy: 0.7834 - val_loss: 0.9498 - val_accuracy: 0.6816 - lr: 9.0000e-04\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6275 - accuracy: 0.7835\n",
      "Epoch 20: val_loss improved from 0.88169 to 0.86795, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.6275 - accuracy: 0.7835 - val_loss: 0.8680 - val_accuracy: 0.7142 - lr: 9.0000e-04\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6273 - accuracy: 0.7862\n",
      "Epoch 21: val_loss improved from 0.86795 to 0.86103, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.6273 - accuracy: 0.7862 - val_loss: 0.8610 - val_accuracy: 0.7222 - lr: 9.0000e-04\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6197 - accuracy: 0.7879\n",
      "Epoch 22: val_loss did not improve from 0.86103\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.6197 - accuracy: 0.7879 - val_loss: 0.8757 - val_accuracy: 0.7116 - lr: 9.0000e-04\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6176 - accuracy: 0.7890\n",
      "Epoch 23: val_loss did not improve from 0.86103\n",
      "1563/1563 [==============================] - 695s 445ms/step - loss: 0.6176 - accuracy: 0.7890 - val_loss: 0.8945 - val_accuracy: 0.7110 - lr: 9.0000e-04\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6077 - accuracy: 0.7935\n",
      "Epoch 24: val_loss did not improve from 0.86103\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "1563/1563 [==============================] - 694s 444ms/step - loss: 0.6077 - accuracy: 0.7935 - val_loss: 0.9101 - val_accuracy: 0.7028 - lr: 9.0000e-04\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6049 - accuracy: 0.7949\n",
      "Epoch 25: val_loss improved from 0.86103 to 0.83671, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.6049 - accuracy: 0.7949 - val_loss: 0.8367 - val_accuracy: 0.7300 - lr: 8.1000e-04\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5947 - accuracy: 0.7986\n",
      "Epoch 26: val_loss improved from 0.83671 to 0.82979, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 695s 445ms/step - loss: 0.5947 - accuracy: 0.7986 - val_loss: 0.8298 - val_accuracy: 0.7282 - lr: 8.1000e-04\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5974 - accuracy: 0.7962\n",
      "Epoch 27: val_loss did not improve from 0.82979\n",
      "1563/1563 [==============================] - 695s 445ms/step - loss: 0.5974 - accuracy: 0.7962 - val_loss: 0.8379 - val_accuracy: 0.7266 - lr: 8.1000e-04\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5919 - accuracy: 0.7976\n",
      "Epoch 28: val_loss did not improve from 0.82979\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.5919 - accuracy: 0.7976 - val_loss: 0.8352 - val_accuracy: 0.7310 - lr: 8.1000e-04\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5979 - accuracy: 0.7973\n",
      "Epoch 29: val_loss did not improve from 0.82979\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "1563/1563 [==============================] - 695s 445ms/step - loss: 0.5979 - accuracy: 0.7973 - val_loss: 0.9013 - val_accuracy: 0.7172 - lr: 8.1000e-04\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5834 - accuracy: 0.8004\n",
      "Epoch 30: val_loss improved from 0.82979 to 0.79182, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.5834 - accuracy: 0.8004 - val_loss: 0.7918 - val_accuracy: 0.7372 - lr: 7.2900e-04\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5856 - accuracy: 0.8013\n",
      "Epoch 31: val_loss did not improve from 0.79182\n",
      "1563/1563 [==============================] - 695s 445ms/step - loss: 0.5856 - accuracy: 0.8013 - val_loss: 0.8456 - val_accuracy: 0.7206 - lr: 7.2900e-04\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.8007\n",
      "Epoch 32: val_loss did not improve from 0.79182\n",
      "1563/1563 [==============================] - 696s 446ms/step - loss: 0.5805 - accuracy: 0.8007 - val_loss: 0.8346 - val_accuracy: 0.7348 - lr: 7.2900e-04\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5691 - accuracy: 0.8098\n",
      "Epoch 33: val_loss did not improve from 0.79182\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5691 - accuracy: 0.8098 - val_loss: 0.8232 - val_accuracy: 0.7344 - lr: 7.2900e-04\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.8093\n",
      "Epoch 34: val_loss did not improve from 0.79182\n",
      "1563/1563 [==============================] - 696s 445ms/step - loss: 0.5607 - accuracy: 0.8093 - val_loss: 0.8143 - val_accuracy: 0.7348 - lr: 6.5610e-04\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5662 - accuracy: 0.8073\n",
      "Epoch 35: val_loss did not improve from 0.79182\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5662 - accuracy: 0.8073 - val_loss: 0.8249 - val_accuracy: 0.7392 - lr: 6.5610e-04\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5612 - accuracy: 0.8090\n",
      "Epoch 36: val_loss did not improve from 0.79182\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5612 - accuracy: 0.8090 - val_loss: 0.8200 - val_accuracy: 0.7356 - lr: 6.5610e-04\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5537 - accuracy: 0.8125\n",
      "Epoch 37: val_loss did not improve from 0.79182\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5537 - accuracy: 0.8125 - val_loss: 0.7976 - val_accuracy: 0.7428 - lr: 5.9049e-04\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5580 - accuracy: 0.8124\n",
      "Epoch 38: val_loss did not improve from 0.79182\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5580 - accuracy: 0.8124 - val_loss: 0.8001 - val_accuracy: 0.7382 - lr: 5.9049e-04\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5514 - accuracy: 0.8132\n",
      "Epoch 39: val_loss improved from 0.79182 to 0.77688, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 698s 447ms/step - loss: 0.5514 - accuracy: 0.8132 - val_loss: 0.7769 - val_accuracy: 0.7560 - lr: 5.9049e-04\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5515 - accuracy: 0.8116\n",
      "Epoch 40: val_loss did not improve from 0.77688\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5515 - accuracy: 0.8116 - val_loss: 0.7936 - val_accuracy: 0.7440 - lr: 5.9049e-04\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5524 - accuracy: 0.8124\n",
      "Epoch 41: val_loss did not improve from 0.77688\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5524 - accuracy: 0.8124 - val_loss: 0.7954 - val_accuracy: 0.7442 - lr: 5.9049e-04\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5455 - accuracy: 0.8144\n",
      "Epoch 42: val_loss did not improve from 0.77688\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
      "1563/1563 [==============================] - 696s 446ms/step - loss: 0.5455 - accuracy: 0.8144 - val_loss: 0.7996 - val_accuracy: 0.7444 - lr: 5.9049e-04\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.8198\n",
      "Epoch 43: val_loss did not improve from 0.77688\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5299 - accuracy: 0.8198 - val_loss: 0.7985 - val_accuracy: 0.7438 - lr: 5.3144e-04\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.8150\n",
      "Epoch 44: val_loss did not improve from 0.77688\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5427 - accuracy: 0.8150 - val_loss: 0.7842 - val_accuracy: 0.7524 - lr: 5.3144e-04\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.8160\n",
      "Epoch 45: val_loss did not improve from 0.77688\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5409 - accuracy: 0.8160 - val_loss: 0.7960 - val_accuracy: 0.7472 - lr: 5.3144e-04\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.8244\n",
      "Epoch 46: val_loss improved from 0.77688 to 0.77585, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5279 - accuracy: 0.8244 - val_loss: 0.7758 - val_accuracy: 0.7566 - lr: 4.7830e-04\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5341 - accuracy: 0.8188\n",
      "Epoch 47: val_loss did not improve from 0.77585\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5341 - accuracy: 0.8188 - val_loss: 0.7879 - val_accuracy: 0.7516 - lr: 4.7830e-04\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5262 - accuracy: 0.8216\n",
      "Epoch 48: val_loss did not improve from 0.77585\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5262 - accuracy: 0.8216 - val_loss: 0.7765 - val_accuracy: 0.7422 - lr: 4.7830e-04\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5218 - accuracy: 0.8226\n",
      "Epoch 49: val_loss did not improve from 0.77585\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5218 - accuracy: 0.8226 - val_loss: 0.7861 - val_accuracy: 0.7482 - lr: 4.7830e-04\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5230 - accuracy: 0.8227\n",
      "Epoch 50: val_loss improved from 0.77585 to 0.77083, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5230 - accuracy: 0.8227 - val_loss: 0.7708 - val_accuracy: 0.7530 - lr: 4.3047e-04\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5220 - accuracy: 0.8239\n",
      "Epoch 51: val_loss improved from 0.77083 to 0.76580, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 698s 446ms/step - loss: 0.5220 - accuracy: 0.8239 - val_loss: 0.7658 - val_accuracy: 0.7558 - lr: 4.3047e-04\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5213 - accuracy: 0.8236\n",
      "Epoch 52: val_loss did not improve from 0.76580\n",
      "1563/1563 [==============================] - 696s 446ms/step - loss: 0.5213 - accuracy: 0.8236 - val_loss: 0.7727 - val_accuracy: 0.7538 - lr: 4.3047e-04\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5218 - accuracy: 0.8237\n",
      "Epoch 53: val_loss did not improve from 0.76580\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5218 - accuracy: 0.8237 - val_loss: 0.7867 - val_accuracy: 0.7446 - lr: 4.3047e-04\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.8274\n",
      "Epoch 54: val_loss did not improve from 0.76580\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5123 - accuracy: 0.8274 - val_loss: 0.7916 - val_accuracy: 0.7400 - lr: 4.3047e-04\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.8257\n",
      "Epoch 55: val_loss improved from 0.76580 to 0.76008, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5123 - accuracy: 0.8257 - val_loss: 0.7601 - val_accuracy: 0.7566 - lr: 3.8742e-04\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5068 - accuracy: 0.8298\n",
      "Epoch 56: val_loss did not improve from 0.76008\n",
      "1563/1563 [==============================] - 696s 446ms/step - loss: 0.5068 - accuracy: 0.8298 - val_loss: 0.7806 - val_accuracy: 0.7504 - lr: 3.8742e-04\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.8247\n",
      "Epoch 57: val_loss did not improve from 0.76008\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5156 - accuracy: 0.8247 - val_loss: 0.7621 - val_accuracy: 0.7602 - lr: 3.8742e-04\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5096 - accuracy: 0.8266\n",
      "Epoch 58: val_loss did not improve from 0.76008\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5096 - accuracy: 0.8266 - val_loss: 0.7863 - val_accuracy: 0.7528 - lr: 3.8742e-04\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.8271\n",
      "Epoch 59: val_loss did not improve from 0.76008\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5076 - accuracy: 0.8271 - val_loss: 0.7815 - val_accuracy: 0.7546 - lr: 3.4868e-04\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.8277\n",
      "Epoch 60: val_loss did not improve from 0.76008\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5076 - accuracy: 0.8277 - val_loss: 0.7724 - val_accuracy: 0.7500 - lr: 3.4868e-04\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5000 - accuracy: 0.8303\n",
      "Epoch 61: val_loss improved from 0.76008 to 0.75867, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5000 - accuracy: 0.8303 - val_loss: 0.7587 - val_accuracy: 0.7586 - lr: 3.4868e-04\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5020 - accuracy: 0.8294\n",
      "Epoch 62: val_loss improved from 0.75867 to 0.75222, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 698s 446ms/step - loss: 0.5020 - accuracy: 0.8294 - val_loss: 0.7522 - val_accuracy: 0.7580 - lr: 3.4868e-04\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5075 - accuracy: 0.8270\n",
      "Epoch 63: val_loss did not improve from 0.75222\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5075 - accuracy: 0.8270 - val_loss: 0.7822 - val_accuracy: 0.7462 - lr: 3.4868e-04\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4956 - accuracy: 0.8336\n",
      "Epoch 64: val_loss did not improve from 0.75222\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4956 - accuracy: 0.8336 - val_loss: 0.7526 - val_accuracy: 0.7608 - lr: 3.4868e-04\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4993 - accuracy: 0.8331\n",
      "Epoch 65: val_loss did not improve from 0.75222\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4993 - accuracy: 0.8331 - val_loss: 0.7892 - val_accuracy: 0.7558 - lr: 3.4868e-04\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4958 - accuracy: 0.8343\n",
      "Epoch 66: val_loss did not improve from 0.75222\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4958 - accuracy: 0.8343 - val_loss: 0.7668 - val_accuracy: 0.7526 - lr: 3.1381e-04\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5019 - accuracy: 0.8308\n",
      "Epoch 67: val_loss did not improve from 0.75222\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.5019 - accuracy: 0.8308 - val_loss: 0.7669 - val_accuracy: 0.7622 - lr: 3.1381e-04\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4881 - accuracy: 0.8348\n",
      "Epoch 68: val_loss did not improve from 0.75222\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4881 - accuracy: 0.8348 - val_loss: 0.7635 - val_accuracy: 0.7624 - lr: 3.1381e-04\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.8318\n",
      "Epoch 69: val_loss improved from 0.75222 to 0.74666, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4969 - accuracy: 0.8318 - val_loss: 0.7467 - val_accuracy: 0.7684 - lr: 2.8243e-04\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4835 - accuracy: 0.8388\n",
      "Epoch 70: val_loss did not improve from 0.74666\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4835 - accuracy: 0.8388 - val_loss: 0.7794 - val_accuracy: 0.7550 - lr: 2.8243e-04\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4887 - accuracy: 0.8350\n",
      "Epoch 71: val_loss did not improve from 0.74666\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4887 - accuracy: 0.8350 - val_loss: 0.7633 - val_accuracy: 0.7522 - lr: 2.8243e-04\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4966 - accuracy: 0.8335\n",
      "Epoch 72: val_loss did not improve from 0.74666\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
      "1563/1563 [==============================] - 696s 446ms/step - loss: 0.4966 - accuracy: 0.8335 - val_loss: 0.7902 - val_accuracy: 0.7492 - lr: 2.8243e-04\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4896 - accuracy: 0.8334\n",
      "Epoch 73: val_loss did not improve from 0.74666\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4896 - accuracy: 0.8334 - val_loss: 0.7630 - val_accuracy: 0.7572 - lr: 2.5419e-04\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4892 - accuracy: 0.8340\n",
      "Epoch 74: val_loss did not improve from 0.74666\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4892 - accuracy: 0.8340 - val_loss: 0.7500 - val_accuracy: 0.7558 - lr: 2.5419e-04\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4861 - accuracy: 0.8367\n",
      "Epoch 75: val_loss improved from 0.74666 to 0.73260, saving model to /home/sidtandon/Sid/GitRepo/iclr-fastshap/fastshap/experiments/images/cifar10/surrogate/20221115_02_04_39/value_weights.h5\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4861 - accuracy: 0.8367 - val_loss: 0.7326 - val_accuracy: 0.7628 - lr: 2.5419e-04\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4842 - accuracy: 0.8378\n",
      "Epoch 76: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4842 - accuracy: 0.8378 - val_loss: 0.7401 - val_accuracy: 0.7636 - lr: 2.5419e-04\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4785 - accuracy: 0.8379\n",
      "Epoch 77: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4785 - accuracy: 0.8379 - val_loss: 0.7776 - val_accuracy: 0.7552 - lr: 2.5419e-04\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4892 - accuracy: 0.8347\n",
      "Epoch 78: val_loss did not improve from 0.73260\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4892 - accuracy: 0.8347 - val_loss: 0.7697 - val_accuracy: 0.7558 - lr: 2.5419e-04\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4862 - accuracy: 0.8354\n",
      "Epoch 79: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4862 - accuracy: 0.8354 - val_loss: 0.7385 - val_accuracy: 0.7690 - lr: 2.2877e-04\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4793 - accuracy: 0.8372\n",
      "Epoch 80: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4793 - accuracy: 0.8372 - val_loss: 0.7550 - val_accuracy: 0.7648 - lr: 2.2877e-04\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4736 - accuracy: 0.8411\n",
      "Epoch 81: val_loss did not improve from 0.73260\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4736 - accuracy: 0.8411 - val_loss: 0.7488 - val_accuracy: 0.7580 - lr: 2.2877e-04\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.8381\n",
      "Epoch 82: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4791 - accuracy: 0.8381 - val_loss: 0.7450 - val_accuracy: 0.7638 - lr: 2.0589e-04\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4753 - accuracy: 0.8405\n",
      "Epoch 83: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 698s 446ms/step - loss: 0.4753 - accuracy: 0.8405 - val_loss: 0.7448 - val_accuracy: 0.7548 - lr: 2.0589e-04\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4729 - accuracy: 0.8406\n",
      "Epoch 84: val_loss did not improve from 0.73260\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 0.00018530203378759326.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4729 - accuracy: 0.8406 - val_loss: 0.7480 - val_accuracy: 0.7642 - lr: 2.0589e-04\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4725 - accuracy: 0.8431\n",
      "Epoch 85: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4725 - accuracy: 0.8431 - val_loss: 0.7411 - val_accuracy: 0.7624 - lr: 1.8530e-04\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4765 - accuracy: 0.8396\n",
      "Epoch 86: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4765 - accuracy: 0.8396 - val_loss: 0.7878 - val_accuracy: 0.7488 - lr: 1.8530e-04\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4733 - accuracy: 0.8393\n",
      "Epoch 87: val_loss did not improve from 0.73260\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.00016677183302817866.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4733 - accuracy: 0.8393 - val_loss: 0.7376 - val_accuracy: 0.7660 - lr: 1.8530e-04\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4707 - accuracy: 0.8425\n",
      "Epoch 88: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4707 - accuracy: 0.8425 - val_loss: 0.7571 - val_accuracy: 0.7588 - lr: 1.6677e-04\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4706 - accuracy: 0.8428\n",
      "Epoch 89: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 698s 446ms/step - loss: 0.4706 - accuracy: 0.8428 - val_loss: 0.7592 - val_accuracy: 0.7614 - lr: 1.6677e-04\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4701 - accuracy: 0.8410\n",
      "Epoch 90: val_loss did not improve from 0.73260\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 0.00015009464841568844.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4701 - accuracy: 0.8410 - val_loss: 0.7380 - val_accuracy: 0.7678 - lr: 1.6677e-04\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4722 - accuracy: 0.8412\n",
      "Epoch 91: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4722 - accuracy: 0.8412 - val_loss: 0.7566 - val_accuracy: 0.7606 - lr: 1.5009e-04\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4672 - accuracy: 0.8442\n",
      "Epoch 92: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4672 - accuracy: 0.8442 - val_loss: 0.7551 - val_accuracy: 0.7610 - lr: 1.5009e-04\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4636 - accuracy: 0.8448\n",
      "Epoch 93: val_loss did not improve from 0.73260\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 0.0001350851875031367.\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4636 - accuracy: 0.8448 - val_loss: 0.7584 - val_accuracy: 0.7578 - lr: 1.5009e-04\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.8440\n",
      "Epoch 94: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 696s 446ms/step - loss: 0.4669 - accuracy: 0.8440 - val_loss: 0.7412 - val_accuracy: 0.7634 - lr: 1.3509e-04\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4614 - accuracy: 0.8446\n",
      "Epoch 95: val_loss did not improve from 0.73260\n",
      "1563/1563 [==============================] - 697s 446ms/step - loss: 0.4614 - accuracy: 0.8446 - val_loss: 0.7345 - val_accuracy: 0.7680 - lr: 1.3509e-04\n"
     ]
    }
   ],
   "source": [
    "surrogate = ImageSurrogate(model, model_dir)\n",
    "\n",
    "t = time.time()\n",
    "surrogate.train(train_data = ds_train, \n",
    "                val_data = ds_val, \n",
    "                max_epochs = 100, \n",
    "                batch_size = 32, \n",
    "                lookback = 20,\n",
    "                lr = 1e-3)\n",
    "training_time = time.time() - t\n",
    "\n",
    "with open(os.path.join(model_dir, 'training_time.pkl'), 'wb') as f:\n",
    "    pickle.dump(training_time, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d631d35",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b00ad1606bbcc2f06f6d0dddae4ba1f9f3abf61f006d5f3ac83db30fb57b2e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
